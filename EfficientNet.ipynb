{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Systematically Studies of model scaling: \n",
    "    * Balancing of Dept, Width and resolution \n",
    "    * Introduces a new scaling method to balance deiensions. \n",
    "    \n",
    "    \n",
    "    * Most common way to scale is depth.\n",
    "    * Measure computational needs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Initial Observation of EfficientNet? \n",
    "The paper studies the effect of scaling different dimensions of a ConvNet on efficiency. It has 3 dimensions, depth of a network (d), width (w) num channels and resolution (r). The initial observation of the paper after fixing a set of convolution operations is that scaling up along different dimensions has a positive impact but saturates as the models get bigger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![saturtion along different dimensions](images/efficientnet_i.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the paper propose? \n",
    "The paper proposes a new method to scale w,d and r in conjunction. The paper performs tests to show that scaling all three diemnsions in conjunctions gives superior results than increasing just one dimension. The paper proposes a method to scale all three parameters in conjuction. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/efficientnet_ii.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the paper propose to find alpha, beta, gamma and phi? \n",
    "The paper for a fixed set of operations aka, first porposes to fix phi to 1 and then perform a small girdsearch to find good apha beta and gamma which satisfy the above equations. After fixing alpha beta andgamma we scale up the network with different values of phi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the paper aim to optimize? \n",
    "The paper is trying to optimize the following: \n",
    "\n",
    "\n",
    "ACC(m) * [FLOPS(m)/T]**w\n",
    "\n",
    "\n",
    "ACC = Accuracy \n",
    "w = hyperparameter -0.07\n",
    "T = Target FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228800"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75*64*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53952"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*64*256 # Generate 256 Channels\n",
    "25*64*3 # Creating a rep for each channel in input \n",
    "\n",
    "3*64*256 + 25*64*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name a few training tricks that the authors use\n",
    "* Inverted Residual Block: \n",
    "    This is the moblenet arch. Aimed at reducing the number of computations. Used depthwise conv combined by pointwise conv. For an image 12x12x3 to create 256 chanels via normal com we get: 5 * 5 * 3(single conv) x 8 * 8 (cover an entire image) x 256 (num chanels needed) VS Depth + Point: 5 * 5 (single conv) x 8 * 8 (cover image) * 3 * 64 (single channel) x 256 (num channels)\n",
    "     \n",
    "* Squeeze and excitation optimization \n",
    "    >This involves creating an attention type weightage. We take a set of channels and calculate the avg pool across each channel. So, a tensor of HxWxC becomes 1x1xC where each chanel is reduced to one value. We apply a 1x1 conv to this avg pooling and then pass it to a sigmoid style activation. Thus each chanel is given a value between 0-1 which is its weightage. The final op is multiplying all values in HxWxC with its corresposning weightage. \n",
    "* AutoAugment \n",
    "* Stochastic Depth \n",
    "    Usually layers are a conv + a residual connection. aka\n",
    "    > l1 = l0 + Param(l0)\n",
    "    But similar to dropout sometimes if we drop Param(l0) and thus l1 = l0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give a brief outline of the params used\n",
    "* lr = 0.2 decays by 0.97 every 2.4 epochs \n",
    "* batch norm momentum 0.99\n",
    "* RMSProp decay 0.9 momentum 0.9 \n",
    "* SiLU\n",
    "* Stochastic Depth prob: 0.8 \n",
    "* drop increases from B0 to B7. \n",
    "* Early stopping on val set from trainset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give a brief outline of the achievements of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
