{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Fix Up Initilization](https://arxiv.org/abs/1901.09321)\n",
    "Trained a 10k layers NN with no normlisation and only careful initilization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".   batch_norm_equations.png  ewml.png\t   prelui.png\r\n",
      "..  batch_norm_mlp.png\t      preluii.png  Variations_Res_Conns.png\r\n"
     ]
    }
   ],
   "source": [
    "! ls -a images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852)\n",
    "Kaiming Initilization Paper\n",
    "#### PreLu: \n",
    "Here `a` is a learnable parameter. \n",
    "\n",
    "<img width=\"500\" src=\"images/preluii.png\" id=\"neuron\"/>\n",
    "<img width=\"500\" src=\"images/prelui.png\" id=\"neuron\"/>\n",
    "\n",
    "## Questions: \n",
    "1. Write the Update Equation and Derivative for `a` when `a` is chanel dependent and chael independent. \n",
    "\n",
    "### Golden Nuggest:\n",
    "    1. On experiments with PreLu and chanelwise Prelu : There are two interesting phenomena in Table  1.   First,  the  first  conv  layer  (conv1)  has  coefficients(0.681 and 0.596) significantly greater than 0.  As the filters of conv1 are mostly Gabor-like filters such as edge or texture detectors, the learned results show that both positiveand negative responses of the filters are respected.  We believe that this is a more economical way of exploiting low-level information, given the limited number of filters (e.g.,64).  Second, for the channel-wise version, the deeper convlayers in general have smaller coefficients. This implies thatthe  activations  gradually  become  “more  nonlinear”  at  in-creasing depths. In other words, the learned model tends tokeep more information in earlier stages and becomes morediscriminative in deeper stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
