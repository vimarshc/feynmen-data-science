{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participating in a Data Heavy Kaggle Tournament"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first prize [here](https://www.kaggle.com/c/landmark-retrieval-2020/data) states that he trained a 100GB model on Google Colab. \n",
    "\n",
    "1. Downloading data has been tricky. Achieveing the sntire thing via the kaggle CLI is not possible for 100GB + Google Buckets. Colab instances are 250GB and after hooking 500GB GCP Bucket I could not perform `mv` from colab t the bucket. \n",
    "\n",
    "2. Had to spin up a server (incurs costs) and transferring data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to ensure least cost while spinning VMs and dumping onto Buckets? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ensure fast connectivity. Speeds rangke from 8MBpS to 50. After speed can balance the longer time. \n",
    "2. After unzipping and pushing to GCP Bucket WILL take long. I'm doing 100GB of Google Landmark Retrival as files have to go one by one. \n",
    "3. When server location was changed to US-Central speeds of 200MBpS was achieved this downloading the data was easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are: \n",
    "    * IOPs? \n",
    "    * DiskThroughput \n",
    "    * Network Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
